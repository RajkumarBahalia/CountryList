{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1kP2SLH9tonW24bxYDTJeAzhRr_WwQLYk",
      "authorship_tag": "ABX9TyMhDfhU92R6T7bnAydpgqK4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajkumarBahalia/CountryList/blob/master/NiftyDailyMarketDepth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGRh2C2NktjO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# =============================\n",
        "# 1. Check if file exists\n",
        "# =============================\n",
        "default_filename = \"nifty_ohlc.csv\"\n",
        "\n",
        "if os.path.exists(default_filename):\n",
        "    print(f\"Found existing file: {default_filename}\")\n",
        "    filename = default_filename\n",
        "else:\n",
        "    print(\"File 'nifty_ohlc.csv' not found. Please upload your NIFTY OHLC CSV file:\")\n",
        "    uploaded = files.upload()\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    print(\"Uploaded file:\", filename)\n",
        "\n",
        "# =============================\n",
        "# 2. Robust CSV Read\n",
        "# =============================\n",
        "try:\n",
        "    df = pd.read_csv(filename, encoding=\"utf-8\")\n",
        "except UnicodeDecodeError:\n",
        "    df = pd.read_csv(filename, encoding=\"ISO-8859-1\")\n",
        "\n",
        "# =============================\n",
        "# 3. Mandatory column check\n",
        "# =============================\n",
        "required_cols = ['Date', 'Close']\n",
        "missing = [col for col in required_cols if col not in df.columns]\n",
        "\n",
        "if missing:\n",
        "    raise Exception(f\"Missing required column(s): {missing}. Please check your CSV file.\")\n",
        "\n",
        "# =============================\n",
        "# 4. Parse date column safely\n",
        "# =============================\n",
        "\n",
        "# Work with Date as string\n",
        "date_raw = df['Date'].astype(str)\n",
        "\n",
        "# First attempt: direct parse\n",
        "parsed_dates = pd.to_datetime(date_raw, errors='coerce')\n",
        "\n",
        "# If everything failed, try cleaning \" GMT...\" suffix (your file case)\n",
        "if parsed_dates.isna().all():\n",
        "    # Remove everything starting from \" GMT\"\n",
        "    date_clean = date_raw.str.split(\" GMT\").str[0]\n",
        "    parsed_dates = pd.to_datetime(date_clean, errors='coerce')\n",
        "\n",
        "df['Date'] = parsed_dates\n",
        "\n",
        "# Drop rows where Date could not be parsed\n",
        "df = df.dropna(subset=['Date'])\n",
        "\n",
        "# If after cleaning there are still no rows, warn\n",
        "if df.empty:\n",
        "    raise Exception(\"After parsing, 'Date' column is empty. Please check the date format in your CSV.\")\n",
        "\n",
        "# Sort by date\n",
        "df = df.sort_values(by='Date')\n",
        "\n",
        "# ============================================\n",
        "# 3. Compute Point Change\n",
        "# ============================================\n",
        "prev_Close = df['Close'].shift(1)\n",
        "change = df['Close'] - prev_Close\n",
        "df['Nifty Chg %'] = ((df['Close'].pct_change()) * 100).round(2)\n",
        "\n",
        "# ============================================\n",
        "# 4. Compute Total Point Advance & Decline\n",
        "# ============================================\n",
        "df['Advance'] = change.apply(lambda x: round(x, 2) if x > 0 else 0)\n",
        "df['Decline'] = change.apply(lambda x: round(x, 2) if x < 0 else 0)\n",
        "\n",
        "# =============================\n",
        "# 5. Calculate Daily % Change\n",
        "# =============================\n",
        "change = (df['Close'].pct_change() * 100).round(2)\n",
        "\n",
        "# 4% Above Positive and 4% Below negative splits\n",
        "df['Up_4%'] = change.apply(lambda x: x if x >= 4 else pd.NA)\n",
        "df['Down_4%'] = change.apply(lambda x: x if x < 4 else pd.NA)\n",
        "\n",
        "df['Up_4%'] = df['Up_4%'].round(2)\n",
        "df['Down_4%'] = df['Down_4%'].round(2)\n",
        "\n",
        "# === Compute Moving Averages ===\n",
        "# === Store DMAs as VARIABLES (not in df) ===\n",
        "dma10  = df['Close'].rolling(10).mean()\n",
        "dma20  = df['Close'].rolling(20).mean()\n",
        "dma50  = df['Close'].rolling(50).mean()\n",
        "dma100 = df['Close'].rolling(100).mean()\n",
        "dma200 = df['Close'].rolling(200).mean()\n",
        "\n",
        "# === Calculate % Above / Below DMA ===\n",
        "df['%_10DMA']  = ((df['Close'] - dma10)  / dma10)  * 100\n",
        "df['%_20DMA']  = ((df['Close'] - dma20)  / dma20)  * 100\n",
        "df['%_50DMA']  = ((df['Close'] - dma50)  / dma50)  * 100\n",
        "df['%_100DMA'] = ((df['Close'] - dma100) / dma100) * 100\n",
        "df['%_200DMA'] = ((df['Close'] - dma200) / dma200) * 100\n",
        "\n",
        "# Round values\n",
        "pct_cols = ['%_10DMA','%_20DMA','%_50DMA','%_100DMA','%_200DMA']\n",
        "df[pct_cols] = df[pct_cols].round(2)\n",
        "\n",
        "output_filename = \"nifty_ohlc_Market_Depth_Data.csv\"\n",
        "df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(\"Saved:\", output_filename)\n"
      ]
    }
  ]
}